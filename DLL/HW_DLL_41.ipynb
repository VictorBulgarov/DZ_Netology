{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15ba6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "import torchvision as tv\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0ddaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "634d1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/wizard/Yandex.Disk.localized/data s/HW/DLL/hymenoptera_data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "745da445",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a008dbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': tv.transforms.Compose([\n",
    "#         tv.transforms.RandomResizedCrop(224),\n",
    "#         tv.transforms.RandomHorizontalFlip(),\n",
    "#         tv.transforms.RandomVerticalFlip(),\n",
    "        tv.transforms.CenterCrop(224),\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': tv.transforms.Compose([\n",
    "        tv.transforms.Resize(256),\n",
    "        tv.transforms.CenterCrop(224),\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: tv.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fecf262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18()\n",
    "vgg16 = models.vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe92969f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wizard/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/wizard/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/wizard/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n",
      "/Users/wizard/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/wizard/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10bbeab",
   "metadata": {},
   "source": [
    "#### 1.Обучите на нем модели ResNet 18 и VGG 16 с нуля (5-10 эпох)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab452e",
   "metadata": {},
   "source": [
    "## ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d39a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83c70341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b09b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, dev):\n",
    "    acc_sum, n = torch.Tensor([0]).to(dev), 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(dev), y.to(dev)\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edc0d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, trainer, num_epochs, dev):\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(dev), y.to(dev)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            print(\"Step. time since epoch: {:.3f}. Train acc: {:.3f}. Train Loss: {:.3f}\".format(time.time() -  start,\n",
    "                (y_hat.argmax(axis=1) == y).sum().item() / y.shape[0], l.item()))\n",
    "        test_acc = evaluate_accuracy(test_iter, net, dev)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "240b917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step. time since epoch: 5.958. Train acc: 0.000. Train Loss: 215.373\n",
      "Step. time since epoch: 8.746. Train acc: 0.500. Train Loss: 182.898\n",
      "Step. time since epoch: 11.442. Train acc: 0.469. Train Loss: 145.546\n",
      "Step. time since epoch: 14.090. Train acc: 0.500. Train Loss: 107.812\n",
      "Step. time since epoch: 16.756. Train acc: 0.750. Train Loss: 58.738\n",
      "Step. time since epoch: 19.562. Train acc: 0.594. Train Loss: 68.560\n",
      "Step. time since epoch: 22.215. Train acc: 0.562. Train Loss: 81.258\n",
      "Step. time since epoch: 23.931. Train acc: 0.750. Train Loss: 20.331\n",
      "epoch 1, loss 3.6087, train acc 0.504, test acc 0.542, time 72.4 sec\n",
      "Step. time since epoch: 5.751. Train acc: 0.594. Train Loss: 426.400\n",
      "Step. time since epoch: 8.444. Train acc: 0.469. Train Loss: 551.599\n",
      "Step. time since epoch: 11.099. Train acc: 0.469. Train Loss: 80.927\n",
      "Step. time since epoch: 13.774. Train acc: 0.438. Train Loss: 141.159\n",
      "Step. time since epoch: 16.421. Train acc: 0.562. Train Loss: 24.820\n",
      "Step. time since epoch: 19.033. Train acc: 0.406. Train Loss: 29.404\n",
      "Step. time since epoch: 21.651. Train acc: 0.438. Train Loss: 136.311\n",
      "Step. time since epoch: 23.528. Train acc: 0.450. Train Loss: 17.275\n",
      "epoch 2, loss 5.7701, train acc 0.480, test acc 0.458, time 71.6 sec\n",
      "Step. time since epoch: 5.428. Train acc: 0.531. Train Loss: 35.130\n",
      "Step. time since epoch: 8.074. Train acc: 0.469. Train Loss: 38.820\n",
      "Step. time since epoch: 10.851. Train acc: 0.438. Train Loss: 31.031\n",
      "Step. time since epoch: 13.465. Train acc: 0.375. Train Loss: 24.009\n",
      "Step. time since epoch: 16.223. Train acc: 0.438. Train Loss: 24.301\n",
      "Step. time since epoch: 18.860. Train acc: 0.312. Train Loss: 22.969\n",
      "Step. time since epoch: 21.625. Train acc: 0.406. Train Loss: 23.113\n",
      "Step. time since epoch: 23.299. Train acc: 0.450. Train Loss: 13.878\n",
      "epoch 3, loss 0.8740, train acc 0.426, test acc 0.458, time 71.4 sec\n",
      "Step. time since epoch: 5.506. Train acc: 0.469. Train Loss: 22.702\n",
      "Step. time since epoch: 8.160. Train acc: 0.500. Train Loss: 22.205\n",
      "Step. time since epoch: 10.946. Train acc: 0.406. Train Loss: 26.598\n",
      "Step. time since epoch: 13.599. Train acc: 0.594. Train Loss: 21.606\n",
      "Step. time since epoch: 16.379. Train acc: 0.688. Train Loss: 22.033\n",
      "Step. time since epoch: 19.017. Train acc: 0.562. Train Loss: 21.982\n",
      "Step. time since epoch: 21.799. Train acc: 0.531. Train Loss: 23.387\n",
      "Step. time since epoch: 23.489. Train acc: 0.550. Train Loss: 13.984\n",
      "epoch 4, loss 0.7152, train acc 0.537, test acc 0.458, time 71.6 sec\n",
      "Step. time since epoch: 5.617. Train acc: 0.531. Train Loss: 21.889\n",
      "Step. time since epoch: 8.290. Train acc: 0.500. Train Loss: 22.103\n",
      "Step. time since epoch: 10.937. Train acc: 0.469. Train Loss: 22.345\n",
      "Step. time since epoch: 13.800. Train acc: 0.375. Train Loss: 23.058\n",
      "Step. time since epoch: 16.430. Train acc: 0.281. Train Loss: 22.474\n",
      "Step. time since epoch: 19.086. Train acc: 0.375. Train Loss: 22.467\n",
      "Step. time since epoch: 21.865. Train acc: 0.625. Train Loss: 21.983\n",
      "Step. time since epoch: 23.560. Train acc: 0.500. Train Loss: 13.629\n",
      "epoch 5, loss 0.6965, train acc 0.455, test acc 0.556, time 71.8 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, dataloaders_dict['train'], dataloaders_dict['val'], trainer, num_epochs, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3429c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release CUDA\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2cf000",
   "metadata": {},
   "source": [
    "## VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81cf8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8047e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d16064e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, dev):\n",
    "    acc_sum, n = torch.Tensor([0]).to(dev), 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(dev), y.to(dev)\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f9fc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, trainer, num_epochs, dev):\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(dev), y.to(dev)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            print(\"Step. time since epoch: {:.3f}. Train acc: {:.3f}. Train Loss: {:.3f}\".format(time.time() -  start,\n",
    "                (y_hat.argmax(axis=1) == y).sum().item() / y.shape[0], l.item()))\n",
    "        test_acc = evaluate_accuracy(test_iter, net, dev)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c76fa533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step. time since epoch: 19.436. Train acc: 0.000. Train Loss: 223.094\n",
      "Step. time since epoch: 35.856. Train acc: 0.469. Train Loss: 4583.928\n",
      "Step. time since epoch: 51.988. Train acc: 0.500. Train Loss: 184.526\n",
      "Step. time since epoch: 68.032. Train acc: 0.469. Train Loss: 620.793\n",
      "Step. time since epoch: 84.269. Train acc: 0.375. Train Loss: 208.773\n",
      "Step. time since epoch: 100.610. Train acc: 0.719. Train Loss: 211.643\n",
      "Step. time since epoch: 116.723. Train acc: 0.531. Train Loss: 209.501\n",
      "Step. time since epoch: 127.642. Train acc: 0.550. Train Loss: 122.799\n",
      "epoch 1, loss 26.0863, train acc 0.447, test acc 0.458, time 191.6 sec\n",
      "Step. time since epoch: 18.840. Train acc: 0.594. Train Loss: 148.030\n",
      "Step. time since epoch: 35.203. Train acc: 0.469. Train Loss: 206.316\n",
      "Step. time since epoch: 51.586. Train acc: 0.531. Train Loss: 102.189\n",
      "Step. time since epoch: 68.041. Train acc: 0.469. Train Loss: 122.686\n",
      "Step. time since epoch: 84.223. Train acc: 0.500. Train Loss: 95.468\n",
      "Step. time since epoch: 100.522. Train acc: 0.500. Train Loss: 90.464\n",
      "Step. time since epoch: 116.865. Train acc: 0.406. Train Loss: 108.719\n",
      "Step. time since epoch: 127.840. Train acc: 0.600. Train Loss: 38.567\n",
      "epoch 2, loss 3.7395, train acc 0.504, test acc 0.458, time 192.4 sec\n",
      "Step. time since epoch: 19.024. Train acc: 0.406. Train Loss: 75.382\n",
      "Step. time since epoch: 35.544. Train acc: 0.594. Train Loss: 54.387\n",
      "Step. time since epoch: 51.814. Train acc: 0.500. Train Loss: 52.207\n",
      "Step. time since epoch: 67.873. Train acc: 0.469. Train Loss: 42.976\n",
      "Step. time since epoch: 84.157. Train acc: 0.625. Train Loss: 26.203\n",
      "Step. time since epoch: 100.433. Train acc: 0.438. Train Loss: 26.813\n",
      "Step. time since epoch: 116.693. Train acc: 0.438. Train Loss: 23.155\n",
      "Step. time since epoch: 127.646. Train acc: 0.600. Train Loss: 14.780\n",
      "epoch 3, loss 1.2947, train acc 0.504, test acc 0.542, time 191.7 sec\n",
      "Step. time since epoch: 18.908. Train acc: 0.375. Train Loss: 42.442\n",
      "Step. time since epoch: 35.422. Train acc: 0.531. Train Loss: 29.093\n",
      "Step. time since epoch: 51.923. Train acc: 0.469. Train Loss: 24.761\n",
      "Step. time since epoch: 68.300. Train acc: 0.625. Train Loss: 21.340\n",
      "Step. time since epoch: 84.435. Train acc: 0.406. Train Loss: 31.898\n",
      "Step. time since epoch: 100.822. Train acc: 0.438. Train Loss: 33.841\n",
      "Step. time since epoch: 117.332. Train acc: 0.406. Train Loss: 32.475\n",
      "Step. time since epoch: 128.145. Train acc: 0.550. Train Loss: 14.413\n",
      "epoch 4, loss 0.9437, train acc 0.471, test acc 0.458, time 192.8 sec\n",
      "Step. time since epoch: 18.905. Train acc: 0.375. Train Loss: 22.836\n",
      "Step. time since epoch: 35.551. Train acc: 0.500. Train Loss: 23.197\n",
      "Step. time since epoch: 51.839. Train acc: 0.375. Train Loss: 29.466\n",
      "Step. time since epoch: 68.311. Train acc: 0.500. Train Loss: 25.858\n",
      "Step. time since epoch: 84.777. Train acc: 0.438. Train Loss: 26.222\n",
      "Step. time since epoch: 100.926. Train acc: 0.469. Train Loss: 23.447\n",
      "Step. time since epoch: 117.249. Train acc: 0.531. Train Loss: 22.128\n",
      "Step. time since epoch: 128.009. Train acc: 0.450. Train Loss: 14.146\n",
      "epoch 5, loss 0.7676, train acc 0.455, test acc 0.458, time 192.0 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, dataloaders_dict['train'], dataloaders_dict['val'], trainer, num_epochs, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a38ca",
   "metadata": {},
   "source": [
    "## 2.Обучите на нем модели ResNet 18 и VGG 16 с использованием FineTuning (5-10 эпох)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147f858",
   "metadata": {},
   "source": [
    "### ResNet 18 pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc9bafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0664d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "607941d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Убираем требование градиента:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac37f470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33f565dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(in_features=512, out_features=2).to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91c02f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f796401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, dev):\n",
    "    acc_sum, n = torch.Tensor([0]).to(dev), 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(dev), y.to(dev)\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3573146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, trainer, num_epochs, dev):\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(dev), y.to(dev)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            print(\"Step. time since epoch: {:.3f}. Train acc: {:.3f}. Train Loss: {:.3f}\".format(time.time() -  start,\n",
    "                (y_hat.argmax(axis=1) == y).sum().item() / y.shape[0], l.item()))\n",
    "        test_acc = evaluate_accuracy(test_iter, net, dev)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4465f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step. time since epoch: 3.728. Train acc: 0.406. Train Loss: 22.600\n",
      "Step. time since epoch: 4.937. Train acc: 0.531. Train Loss: 21.821\n",
      "Step. time since epoch: 6.124. Train acc: 0.625. Train Loss: 20.443\n",
      "Step. time since epoch: 7.301. Train acc: 0.594. Train Loss: 23.329\n",
      "Step. time since epoch: 8.480. Train acc: 0.656. Train Loss: 19.574\n",
      "Step. time since epoch: 9.651. Train acc: 0.781. Train Loss: 17.721\n",
      "Step. time since epoch: 10.837. Train acc: 0.562. Train Loss: 19.666\n",
      "Step. time since epoch: 11.622. Train acc: 0.650. Train Loss: 12.380\n",
      "epoch 1, loss 0.6456, train acc 0.598, test acc 0.745, time 59.6 sec\n",
      "Step. time since epoch: 3.776. Train acc: 0.688. Train Loss: 16.058\n",
      "Step. time since epoch: 5.028. Train acc: 0.812. Train Loss: 14.684\n",
      "Step. time since epoch: 6.259. Train acc: 0.812. Train Loss: 13.618\n",
      "Step. time since epoch: 7.408. Train acc: 0.844. Train Loss: 14.268\n",
      "Step. time since epoch: 8.591. Train acc: 0.938. Train Loss: 11.488\n",
      "Step. time since epoch: 9.775. Train acc: 0.875. Train Loss: 12.102\n",
      "Step. time since epoch: 10.937. Train acc: 0.906. Train Loss: 11.043\n",
      "Step. time since epoch: 11.694. Train acc: 0.850. Train Loss: 7.773\n",
      "epoch 2, loss 0.4141, train acc 0.840, test acc 0.843, time 60.1 sec\n",
      "Step. time since epoch: 3.920. Train acc: 0.938. Train Loss: 8.833\n",
      "Step. time since epoch: 5.113. Train acc: 0.875. Train Loss: 11.367\n",
      "Step. time since epoch: 6.285. Train acc: 0.938. Train Loss: 11.368\n",
      "Step. time since epoch: 7.456. Train acc: 0.812. Train Loss: 10.351\n",
      "Step. time since epoch: 8.694. Train acc: 0.906. Train Loss: 9.198\n",
      "Step. time since epoch: 9.911. Train acc: 1.000. Train Loss: 7.962\n",
      "Step. time since epoch: 11.063. Train acc: 0.969. Train Loss: 7.743\n",
      "Step. time since epoch: 11.822. Train acc: 0.950. Train Loss: 5.298\n",
      "epoch 3, loss 0.2956, train acc 0.922, test acc 0.935, time 59.9 sec\n",
      "Step. time since epoch: 3.640. Train acc: 0.969. Train Loss: 6.635\n",
      "Step. time since epoch: 4.819. Train acc: 1.000. Train Loss: 5.830\n",
      "Step. time since epoch: 5.966. Train acc: 0.906. Train Loss: 9.228\n",
      "Step. time since epoch: 7.126. Train acc: 0.875. Train Loss: 9.262\n",
      "Step. time since epoch: 8.278. Train acc: 0.969. Train Loss: 7.747\n",
      "Step. time since epoch: 9.428. Train acc: 0.938. Train Loss: 6.875\n",
      "Step. time since epoch: 10.585. Train acc: 0.938. Train Loss: 7.102\n",
      "Step. time since epoch: 11.345. Train acc: 0.950. Train Loss: 3.885\n",
      "epoch 4, loss 0.2318, train acc 0.943, test acc 0.941, time 59.4 sec\n",
      "Step. time since epoch: 3.640. Train acc: 0.938. Train Loss: 6.799\n",
      "Step. time since epoch: 4.830. Train acc: 0.969. Train Loss: 6.299\n",
      "Step. time since epoch: 5.992. Train acc: 0.969. Train Loss: 4.895\n",
      "Step. time since epoch: 7.148. Train acc: 0.969. Train Loss: 7.228\n",
      "Step. time since epoch: 8.305. Train acc: 0.969. Train Loss: 5.672\n",
      "Step. time since epoch: 9.463. Train acc: 0.938. Train Loss: 6.364\n",
      "Step. time since epoch: 10.620. Train acc: 1.000. Train Loss: 4.435\n",
      "Step. time since epoch: 11.386. Train acc: 1.000. Train Loss: 2.989\n",
      "epoch 5, loss 0.1831, train acc 0.967, test acc 0.954, time 59.7 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, dataloaders_dict['train'], dataloaders_dict['val'], trainer, num_epochs, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ab6a3",
   "metadata": {},
   "source": [
    "### VGG 16 pretrained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e23ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61f0877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de9b3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Убираем требование градиента:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0ac59d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bd30af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[6] = nn.Linear(in_features=4096, out_features=2).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "945373c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96569b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, dev):\n",
    "    acc_sum, n = torch.Tensor([0]).to(dev), 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(dev), y.to(dev)\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2884fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, trainer, num_epochs, dev):\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(dev), y.to(dev)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            print(\"Step. time since epoch: {:.3f}. Train acc: {:.3f}. Train Loss: {:.3f}\".format(time.time() -  start,\n",
    "                (y_hat.argmax(axis=1) == y).sum().item() / y.shape[0], l.item()))\n",
    "        test_acc = evaluate_accuracy(test_iter, net, dev)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58de50c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step. time since epoch: 7.073. Train acc: 0.469. Train Loss: 27.675\n",
      "Step. time since epoch: 11.456. Train acc: 0.656. Train Loss: 18.629\n",
      "Step. time since epoch: 15.782. Train acc: 0.750. Train Loss: 14.478\n",
      "Step. time since epoch: 19.870. Train acc: 0.812. Train Loss: 12.869\n",
      "Step. time since epoch: 24.008. Train acc: 0.969. Train Loss: 8.696\n",
      "Step. time since epoch: 28.344. Train acc: 0.969. Train Loss: 6.461\n",
      "Step. time since epoch: 32.680. Train acc: 0.969. Train Loss: 5.082\n",
      "Step. time since epoch: 35.859. Train acc: 0.900. Train Loss: 4.712\n",
      "epoch 1, loss 0.4041, train acc 0.807, test acc 0.948, time 99.5 sec\n",
      "Step. time since epoch: 6.716. Train acc: 0.938. Train Loss: 5.628\n",
      "Step. time since epoch: 10.851. Train acc: 1.000. Train Loss: 2.393\n",
      "Step. time since epoch: 14.945. Train acc: 0.938. Train Loss: 4.882\n",
      "Step. time since epoch: 19.127. Train acc: 0.969. Train Loss: 3.151\n",
      "Step. time since epoch: 23.400. Train acc: 0.938. Train Loss: 3.356\n",
      "Step. time since epoch: 27.823. Train acc: 1.000. Train Loss: 1.292\n",
      "Step. time since epoch: 31.975. Train acc: 1.000. Train Loss: 2.407\n",
      "Step. time since epoch: 34.893. Train acc: 0.950. Train Loss: 2.435\n",
      "epoch 2, loss 0.1047, train acc 0.967, test acc 0.961, time 97.7 sec\n",
      "Step. time since epoch: 6.763. Train acc: 1.000. Train Loss: 1.028\n",
      "Step. time since epoch: 11.006. Train acc: 1.000. Train Loss: 1.686\n",
      "Step. time since epoch: 15.212. Train acc: 0.969. Train Loss: 2.143\n",
      "Step. time since epoch: 19.357. Train acc: 1.000. Train Loss: 1.794\n",
      "Step. time since epoch: 23.502. Train acc: 0.969. Train Loss: 2.398\n",
      "Step. time since epoch: 27.637. Train acc: 1.000. Train Loss: 1.614\n",
      "Step. time since epoch: 31.771. Train acc: 0.969. Train Loss: 1.992\n",
      "Step. time since epoch: 34.684. Train acc: 0.950. Train Loss: 2.798\n",
      "epoch 3, loss 0.0633, train acc 0.984, test acc 0.961, time 97.7 sec\n",
      "Step. time since epoch: 7.012. Train acc: 1.000. Train Loss: 2.026\n",
      "Step. time since epoch: 11.183. Train acc: 0.969. Train Loss: 1.679\n",
      "Step. time since epoch: 15.388. Train acc: 1.000. Train Loss: 0.500\n",
      "Step. time since epoch: 19.540. Train acc: 0.969. Train Loss: 1.116\n",
      "Step. time since epoch: 23.723. Train acc: 1.000. Train Loss: 1.377\n",
      "Step. time since epoch: 27.881. Train acc: 1.000. Train Loss: 0.758\n",
      "Step. time since epoch: 32.046. Train acc: 0.969. Train Loss: 2.862\n",
      "Step. time since epoch: 34.987. Train acc: 1.000. Train Loss: 0.720\n",
      "epoch 4, loss 0.0452, train acc 0.988, test acc 0.967, time 98.2 sec\n",
      "Step. time since epoch: 7.075. Train acc: 1.000. Train Loss: 0.978\n",
      "Step. time since epoch: 11.221. Train acc: 1.000. Train Loss: 0.518\n",
      "Step. time since epoch: 15.446. Train acc: 0.969. Train Loss: 2.338\n",
      "Step. time since epoch: 19.904. Train acc: 1.000. Train Loss: 1.828\n",
      "Step. time since epoch: 24.145. Train acc: 1.000. Train Loss: 0.683\n",
      "Step. time since epoch: 28.469. Train acc: 1.000. Train Loss: 0.560\n",
      "Step. time since epoch: 32.623. Train acc: 1.000. Train Loss: 0.964\n",
      "Step. time since epoch: 35.572. Train acc: 1.000. Train Loss: 0.275\n",
      "epoch 5, loss 0.0334, train acc 0.996, test acc 0.961, time 98.5 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, dataloaders_dict['train'], dataloaders_dict['val'], trainer, num_epochs, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00adcc",
   "metadata": {},
   "source": [
    "### 3.Добавьте аугментацию данных к пункту 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a99710e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': tv.transforms.Compose([\n",
    "        tv.transforms.RandomResizedCrop(224),\n",
    "        tv.transforms.RandomHorizontalFlip(),\n",
    "        tv.transforms.RandomVerticalFlip(),\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': tv.transforms.Compose([\n",
    "        tv.transforms.Resize(256),\n",
    "        tv.transforms.CenterCrop(224),\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: tv.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61049609",
   "metadata": {},
   "source": [
    "### ResNet 18 pretrained + aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "140b6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a77313e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0119d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Убираем требование градиента:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b93a6093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8de9f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(in_features=512, out_features=2).to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93119216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a2ac3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, dev):\n",
    "    acc_sum, n = torch.Tensor([0]).to(dev), 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(dev), y.to(dev)\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7a24661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, trainer, num_epochs, dev):\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(dev), y.to(dev)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            print(\"Step. time since epoch: {:.3f}. Train acc: {:.3f}. Train Loss: {:.3f}\".format(time.time() -  start,\n",
    "                (y_hat.argmax(axis=1) == y).sum().item() / y.shape[0], l.item()))\n",
    "        test_acc = evaluate_accuracy(test_iter, net, dev)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3dade7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step. time since epoch: 4.014. Train acc: 0.531. Train Loss: 32.026\n",
      "Step. time since epoch: 5.252. Train acc: 0.531. Train Loss: 24.717\n",
      "Step. time since epoch: 6.471. Train acc: 0.344. Train Loss: 24.991\n",
      "Step. time since epoch: 7.684. Train acc: 0.438. Train Loss: 23.758\n",
      "Step. time since epoch: 8.883. Train acc: 0.469. Train Loss: 25.052\n",
      "Step. time since epoch: 10.069. Train acc: 0.531. Train Loss: 25.121\n",
      "Step. time since epoch: 11.257. Train acc: 0.500. Train Loss: 24.369\n",
      "Step. time since epoch: 12.046. Train acc: 0.500. Train Loss: 15.275\n",
      "epoch 1, loss 0.8004, train acc 0.480, test acc 0.719, time 60.1 sec\n",
      "Step. time since epoch: 3.667. Train acc: 0.531. Train Loss: 21.594\n",
      "Step. time since epoch: 4.844. Train acc: 0.656. Train Loss: 20.022\n",
      "Step. time since epoch: 5.994. Train acc: 0.594. Train Loss: 19.602\n",
      "Step. time since epoch: 7.144. Train acc: 0.625. Train Loss: 19.760\n",
      "Step. time since epoch: 8.290. Train acc: 0.531. Train Loss: 20.272\n",
      "Step. time since epoch: 9.441. Train acc: 0.625. Train Loss: 20.780\n",
      "Step. time since epoch: 10.590. Train acc: 0.719. Train Loss: 16.803\n",
      "Step. time since epoch: 11.352. Train acc: 0.850. Train Loss: 10.277\n",
      "epoch 2, loss 0.6111, train acc 0.631, test acc 0.876, time 59.4 sec\n",
      "Step. time since epoch: 3.690. Train acc: 0.906. Train Loss: 14.042\n",
      "Step. time since epoch: 4.867. Train acc: 0.875. Train Loss: 13.492\n",
      "Step. time since epoch: 6.088. Train acc: 0.812. Train Loss: 14.180\n",
      "Step. time since epoch: 7.337. Train acc: 0.750. Train Loss: 16.034\n",
      "Step. time since epoch: 8.523. Train acc: 0.625. Train Loss: 19.640\n",
      "Step. time since epoch: 9.670. Train acc: 0.625. Train Loss: 19.860\n",
      "Step. time since epoch: 10.827. Train acc: 0.906. Train Loss: 11.168\n",
      "Step. time since epoch: 11.592. Train acc: 0.850. Train Loss: 8.026\n",
      "epoch 3, loss 0.4772, train acc 0.791, test acc 0.922, time 59.6 sec\n",
      "Step. time since epoch: 3.672. Train acc: 0.812. Train Loss: 12.167\n",
      "Step. time since epoch: 4.870. Train acc: 0.906. Train Loss: 13.104\n",
      "Step. time since epoch: 6.037. Train acc: 0.875. Train Loss: 12.319\n",
      "Step. time since epoch: 7.201. Train acc: 0.812. Train Loss: 12.176\n",
      "Step. time since epoch: 8.366. Train acc: 0.969. Train Loss: 9.597\n",
      "Step. time since epoch: 9.536. Train acc: 0.906. Train Loss: 12.195\n",
      "Step. time since epoch: 10.702. Train acc: 0.875. Train Loss: 10.395\n",
      "Step. time since epoch: 11.471. Train acc: 0.900. Train Loss: 6.276\n",
      "epoch 4, loss 0.3616, train acc 0.881, test acc 0.869, time 59.6 sec\n",
      "Step. time since epoch: 3.764. Train acc: 0.844. Train Loss: 10.874\n",
      "Step. time since epoch: 4.964. Train acc: 0.844. Train Loss: 12.967\n",
      "Step. time since epoch: 6.123. Train acc: 0.969. Train Loss: 8.319\n",
      "Step. time since epoch: 7.299. Train acc: 0.844. Train Loss: 12.323\n",
      "Step. time since epoch: 8.456. Train acc: 0.938. Train Loss: 10.124\n",
      "Step. time since epoch: 9.609. Train acc: 0.906. Train Loss: 9.976\n",
      "Step. time since epoch: 10.765. Train acc: 0.938. Train Loss: 9.217\n",
      "Step. time since epoch: 11.529. Train acc: 0.950. Train Loss: 5.751\n",
      "epoch 5, loss 0.3260, train acc 0.902, test acc 0.967, time 59.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, dataloaders_dict['train'], dataloaders_dict['val'], trainer, num_epochs, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6396ce1",
   "metadata": {},
   "source": [
    "### VGG 16 pretrained + aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "50878d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c760107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca4c48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Убираем требование градиента:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56af436f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38ba50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[6] = nn.Linear(in_features=4096, out_features=2).to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bac99e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40ea0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, dev):\n",
    "    acc_sum, n = torch.Tensor([0]).to(dev), 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(dev), y.to(dev)\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30ab2489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, trainer, num_epochs, dev):\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(dev), y.to(dev)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            print(\"Step. time since epoch: {:.3f}. Train acc: {:.3f}. Train Loss: {:.3f}\".format(time.time() -  start,\n",
    "                (y_hat.argmax(axis=1) == y).sum().item() / y.shape[0], l.item()))\n",
    "        test_acc = evaluate_accuracy(test_iter, net, dev)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf6e0715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step. time since epoch: 7.728. Train acc: 0.500. Train Loss: 23.961\n",
      "Step. time since epoch: 11.892. Train acc: 0.594. Train Loss: 20.867\n",
      "Step. time since epoch: 16.390. Train acc: 0.688. Train Loss: 16.852\n",
      "Step. time since epoch: 20.792. Train acc: 0.875. Train Loss: 10.949\n",
      "Step. time since epoch: 25.003. Train acc: 0.875. Train Loss: 10.456\n",
      "Step. time since epoch: 29.478. Train acc: 0.938. Train Loss: 9.403\n",
      "Step. time since epoch: 33.688. Train acc: 0.906. Train Loss: 8.298\n",
      "Step. time since epoch: 36.958. Train acc: 0.900. Train Loss: 5.571\n",
      "epoch 1, loss 0.4359, train acc 0.779, test acc 0.954, time 102.2 sec\n",
      "Step. time since epoch: 7.027. Train acc: 1.000. Train Loss: 3.968\n",
      "Step. time since epoch: 11.189. Train acc: 0.969. Train Loss: 4.746\n",
      "Step. time since epoch: 15.348. Train acc: 1.000. Train Loss: 4.745\n",
      "Step. time since epoch: 19.456. Train acc: 0.969. Train Loss: 4.866\n",
      "Step. time since epoch: 23.570. Train acc: 0.875. Train Loss: 7.522\n",
      "Step. time since epoch: 27.678. Train acc: 0.969. Train Loss: 2.843\n",
      "Step. time since epoch: 31.787. Train acc: 0.969. Train Loss: 2.943\n",
      "Step. time since epoch: 34.688. Train acc: 0.950. Train Loss: 2.260\n",
      "epoch 2, loss 0.1389, train acc 0.963, test acc 0.954, time 97.4 sec\n",
      "Step. time since epoch: 7.156. Train acc: 1.000. Train Loss: 2.294\n",
      "Step. time since epoch: 11.283. Train acc: 0.969. Train Loss: 5.255\n",
      "Step. time since epoch: 15.432. Train acc: 0.938. Train Loss: 5.360\n",
      "Step. time since epoch: 19.615. Train acc: 0.969. Train Loss: 4.825\n",
      "Step. time since epoch: 23.964. Train acc: 1.000. Train Loss: 1.868\n",
      "Step. time since epoch: 28.836. Train acc: 0.938. Train Loss: 3.614\n",
      "Step. time since epoch: 33.287. Train acc: 0.938. Train Loss: 5.708\n",
      "Step. time since epoch: 36.205. Train acc: 0.900. Train Loss: 2.088\n",
      "epoch 3, loss 0.1271, train acc 0.959, test acc 0.954, time 99.3 sec\n",
      "Step. time since epoch: 7.034. Train acc: 1.000. Train Loss: 1.378\n",
      "Step. time since epoch: 11.172. Train acc: 0.969. Train Loss: 1.878\n",
      "Step. time since epoch: 15.328. Train acc: 0.969. Train Loss: 4.580\n",
      "Step. time since epoch: 19.440. Train acc: 0.875. Train Loss: 8.589\n",
      "Step. time since epoch: 23.574. Train acc: 0.969. Train Loss: 3.052\n",
      "Step. time since epoch: 27.705. Train acc: 0.938. Train Loss: 4.110\n",
      "Step. time since epoch: 31.820. Train acc: 0.969. Train Loss: 4.109\n",
      "Step. time since epoch: 34.740. Train acc: 0.950. Train Loss: 4.275\n",
      "epoch 4, loss 0.1310, train acc 0.955, test acc 0.948, time 97.5 sec\n",
      "Step. time since epoch: 7.080. Train acc: 0.875. Train Loss: 5.748\n",
      "Step. time since epoch: 11.216. Train acc: 1.000. Train Loss: 3.600\n",
      "Step. time since epoch: 15.408. Train acc: 0.969. Train Loss: 2.644\n",
      "Step. time since epoch: 19.538. Train acc: 0.969. Train Loss: 1.948\n",
      "Step. time since epoch: 23.662. Train acc: 0.938. Train Loss: 4.809\n",
      "Step. time since epoch: 27.793. Train acc: 0.938. Train Loss: 6.948\n",
      "Step. time since epoch: 31.910. Train acc: 0.906. Train Loss: 5.396\n",
      "Step. time since epoch: 34.820. Train acc: 1.000. Train Loss: 1.312\n",
      "epoch 5, loss 0.1328, train acc 0.947, test acc 0.948, time 97.7 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, dataloaders_dict['train'], dataloaders_dict['val'], trainer, num_epochs, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9d035",
   "metadata": {},
   "source": [
    "## Сравните качество всех 3 полученных подходов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd63c1",
   "metadata": {},
   "source": [
    "Качество обучения на тестовой и валидационной выборках в случае finetuning оказалось наилучшим. При этом предполагаем, что с учетом аугментации данных модель становится стабильнее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6365a3",
   "metadata": {},
   "source": [
    "## Примените FineTuning ResNet 18 к FashionMnist. Удалось ли увидеть резкое увеличение качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "da0c7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eeef4a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transoforms = tv.transforms.Compose([\n",
    "    tv.transforms.Grayscale(3),\n",
    "    tv.transforms.Resize((224,224)),\n",
    "    tv.transforms.ToTensor()\n",
    "])\n",
    "train_dataset = tv.datasets.MNIST('.', train=True, transform=transoforms, download=True)\n",
    "test_dataset = tv.datasets.MNIST('.', train=False, transform=transoforms, download=True)\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dfd8d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tv.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b16dcf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "adfda33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Убираем требование градиента:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b24136ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7165ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(in_features=512, out_features=10).to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41516240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82f46bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, dev):\n",
    "    acc_sum, n = torch.Tensor([0]).to(dev), 0\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(dev), y.to(dev)\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5dcb21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, trainer, num_epochs, dev):\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(dev), y.to(dev)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            print(\"Step. time since epoch: {:.3f}. Train acc: {:.3f}. Train Loss: {:.3f}\".format(time.time() -  start,\n",
    "                (y_hat.argmax(axis=1) == y).sum().item() / y.shape[0], l.item()))\n",
    "        test_acc = evaluate_accuracy(test_iter, net, dev)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a0da021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step. time since epoch: 9.134. Train acc: 0.078. Train Loss: 642.061\n",
      "Step. time since epoch: 18.045. Train acc: 0.102. Train Loss: 609.979\n",
      "Step. time since epoch: 26.910. Train acc: 0.145. Train Loss: 593.892\n",
      "Step. time since epoch: 35.787. Train acc: 0.188. Train Loss: 572.288\n",
      "Step. time since epoch: 44.634. Train acc: 0.238. Train Loss: 564.916\n",
      "Step. time since epoch: 53.510. Train acc: 0.309. Train Loss: 535.893\n",
      "Step. time since epoch: 62.475. Train acc: 0.352. Train Loss: 512.122\n",
      "Step. time since epoch: 71.335. Train acc: 0.395. Train Loss: 503.749\n",
      "Step. time since epoch: 80.255. Train acc: 0.461. Train Loss: 477.669\n",
      "Step. time since epoch: 89.077. Train acc: 0.492. Train Loss: 477.212\n",
      "Step. time since epoch: 97.966. Train acc: 0.535. Train Loss: 461.547\n",
      "Step. time since epoch: 106.865. Train acc: 0.574. Train Loss: 438.375\n",
      "Step. time since epoch: 115.684. Train acc: 0.578. Train Loss: 435.309\n",
      "Step. time since epoch: 124.578. Train acc: 0.578. Train Loss: 422.826\n",
      "Step. time since epoch: 133.482. Train acc: 0.668. Train Loss: 402.613\n",
      "Step. time since epoch: 142.351. Train acc: 0.660. Train Loss: 393.417\n",
      "Step. time since epoch: 151.157. Train acc: 0.699. Train Loss: 379.936\n",
      "Step. time since epoch: 160.045. Train acc: 0.695. Train Loss: 364.780\n",
      "Step. time since epoch: 168.939. Train acc: 0.676. Train Loss: 370.258\n",
      "Step. time since epoch: 177.864. Train acc: 0.734. Train Loss: 346.505\n",
      "Step. time since epoch: 186.842. Train acc: 0.746. Train Loss: 337.949\n",
      "Step. time since epoch: 195.752. Train acc: 0.754. Train Loss: 328.751\n",
      "Step. time since epoch: 204.669. Train acc: 0.766. Train Loss: 328.165\n",
      "Step. time since epoch: 213.563. Train acc: 0.738. Train Loss: 335.431\n",
      "Step. time since epoch: 222.458. Train acc: 0.738. Train Loss: 314.966\n",
      "Step. time since epoch: 231.420. Train acc: 0.785. Train Loss: 285.909\n",
      "Step. time since epoch: 240.331. Train acc: 0.805. Train Loss: 294.299\n",
      "Step. time since epoch: 249.270. Train acc: 0.785. Train Loss: 274.574\n",
      "Step. time since epoch: 258.241. Train acc: 0.820. Train Loss: 289.341\n",
      "Step. time since epoch: 267.169. Train acc: 0.832. Train Loss: 266.796\n",
      "Step. time since epoch: 276.138. Train acc: 0.797. Train Loss: 280.628\n",
      "Step. time since epoch: 285.114. Train acc: 0.801. Train Loss: 250.963\n",
      "Step. time since epoch: 294.102. Train acc: 0.844. Train Loss: 259.310\n",
      "Step. time since epoch: 303.139. Train acc: 0.770. Train Loss: 286.672\n",
      "Step. time since epoch: 312.243. Train acc: 0.809. Train Loss: 266.928\n",
      "Step. time since epoch: 321.192. Train acc: 0.914. Train Loss: 197.943\n",
      "Step. time since epoch: 330.102. Train acc: 0.859. Train Loss: 230.446\n",
      "Step. time since epoch: 338.978. Train acc: 0.832. Train Loss: 228.761\n",
      "Step. time since epoch: 347.896. Train acc: 0.875. Train Loss: 197.420\n",
      "Step. time since epoch: 356.814. Train acc: 0.824. Train Loss: 227.599\n",
      "Step. time since epoch: 365.746. Train acc: 0.852. Train Loss: 227.825\n",
      "Step. time since epoch: 374.664. Train acc: 0.875. Train Loss: 196.791\n",
      "Step. time since epoch: 383.612. Train acc: 0.883. Train Loss: 203.587\n",
      "Step. time since epoch: 392.496. Train acc: 0.832. Train Loss: 206.930\n",
      "Step. time since epoch: 401.452. Train acc: 0.875. Train Loss: 189.370\n",
      "Step. time since epoch: 410.383. Train acc: 0.898. Train Loss: 190.676\n",
      "Step. time since epoch: 419.324. Train acc: 0.867. Train Loss: 196.219\n",
      "Step. time since epoch: 428.309. Train acc: 0.867. Train Loss: 187.860\n",
      "Step. time since epoch: 437.232. Train acc: 0.918. Train Loss: 183.628\n",
      "Step. time since epoch: 446.139. Train acc: 0.863. Train Loss: 188.134\n",
      "Step. time since epoch: 455.387. Train acc: 0.840. Train Loss: 203.601\n",
      "Step. time since epoch: 464.803. Train acc: 0.859. Train Loss: 201.338\n",
      "Step. time since epoch: 473.701. Train acc: 0.906. Train Loss: 161.865\n",
      "Step. time since epoch: 482.602. Train acc: 0.902. Train Loss: 163.706\n",
      "Step. time since epoch: 492.299. Train acc: 0.848. Train Loss: 187.232\n",
      "Step. time since epoch: 501.353. Train acc: 0.871. Train Loss: 173.217\n",
      "Step. time since epoch: 510.825. Train acc: 0.832. Train Loss: 191.534\n",
      "Step. time since epoch: 519.769. Train acc: 0.871. Train Loss: 172.647\n",
      "Step. time since epoch: 528.732. Train acc: 0.883. Train Loss: 158.021\n",
      "Step. time since epoch: 537.666. Train acc: 0.898. Train Loss: 140.879\n",
      "Step. time since epoch: 546.587. Train acc: 0.898. Train Loss: 152.087\n",
      "Step. time since epoch: 555.515. Train acc: 0.859. Train Loss: 179.094\n",
      "Step. time since epoch: 564.454. Train acc: 0.852. Train Loss: 182.061\n",
      "Step. time since epoch: 573.342. Train acc: 0.902. Train Loss: 135.581\n",
      "Step. time since epoch: 582.264. Train acc: 0.934. Train Loss: 115.209\n",
      "Step. time since epoch: 591.312. Train acc: 0.898. Train Loss: 152.554\n",
      "Step. time since epoch: 600.410. Train acc: 0.867. Train Loss: 163.225\n",
      "Step. time since epoch: 609.935. Train acc: 0.910. Train Loss: 147.109\n",
      "Step. time since epoch: 619.008. Train acc: 0.867. Train Loss: 174.636\n",
      "Step. time since epoch: 627.992. Train acc: 0.863. Train Loss: 164.816\n",
      "Step. time since epoch: 637.048. Train acc: 0.957. Train Loss: 108.928\n",
      "Step. time since epoch: 646.118. Train acc: 0.871. Train Loss: 133.323\n",
      "Step. time since epoch: 655.720. Train acc: 0.902. Train Loss: 132.440\n",
      "Step. time since epoch: 665.249. Train acc: 0.926. Train Loss: 115.676\n",
      "Step. time since epoch: 674.069. Train acc: 0.930. Train Loss: 126.355\n",
      "Step. time since epoch: 683.061. Train acc: 0.910. Train Loss: 133.646\n",
      "Step. time since epoch: 693.814. Train acc: 0.895. Train Loss: 130.441\n",
      "Step. time since epoch: 703.992. Train acc: 0.898. Train Loss: 135.390\n",
      "Step. time since epoch: 713.463. Train acc: 0.883. Train Loss: 139.394\n",
      "Step. time since epoch: 722.481. Train acc: 0.922. Train Loss: 122.822\n",
      "Step. time since epoch: 732.109. Train acc: 0.926. Train Loss: 123.444\n",
      "Step. time since epoch: 741.294. Train acc: 0.895. Train Loss: 133.992\n",
      "Step. time since epoch: 750.770. Train acc: 0.926. Train Loss: 114.528\n",
      "Step. time since epoch: 760.619. Train acc: 0.906. Train Loss: 120.116\n",
      "Step. time since epoch: 770.319. Train acc: 0.934. Train Loss: 94.501\n",
      "Step. time since epoch: 780.620. Train acc: 0.922. Train Loss: 112.586\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [78], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lr, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [77], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, test_iter, trainer, num_epochs, dev)\u001b[0m\n\u001b[1;32m      7\u001b[0m trainer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(dev), y\u001b[38;5;241m.\u001b[39mto(dev)\n\u001b[0;32m----> 9\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(y_hat, y)\n\u001b[1;32m     11\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/models/resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 1\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, train_iter, test_iter, trainer, num_epochs, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a771d3d9",
   "metadata": {},
   "source": [
    "хорошо обучается и выходит за 90-92% на сравнительном быстром периоде обучения, чем обычная сетка, но в определенный момент начинает переобучаться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68823949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
