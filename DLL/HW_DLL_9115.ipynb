{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решить задачу детекции на основе SSD для датасета https://github.com/Shenggan/BCCD_Dataset\n",
    "Реализацию SSD можно подглядеть тут - https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection\n",
    "\n",
    "Cтатья по SSD https://d2l.ai/chapter_computer-vision/ssd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "from itertools import product as product\n",
    "import torchvision\n",
    "import os, sys, random\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from shutil import copyfile\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as FT\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = glob('BCCD/Annotations/*.xml')\n",
    "\n",
    "df = []\n",
    "cnt = 0\n",
    "for file in annotations:\n",
    "    #filename = file.split('/')[-1].split('.')[0] + '.jpg'\n",
    "    #filename = str(cnt) + '.jpg'\n",
    "    filename = file.split('\\\\')[-1]\n",
    "    filename =filename.split('.')[0] + '.jpg'\n",
    "    row = []\n",
    "    parsedXML = ET.parse(file)\n",
    "    for node in parsedXML.getroot().iter('object'):\n",
    "        blood_cells = node.find('name').text\n",
    "        xmin = int(node.find('bndbox/xmin').text)\n",
    "        xmax = int(node.find('bndbox/xmax').text)\n",
    "        ymin = int(node.find('bndbox/ymin').text)\n",
    "        ymax = int(node.find('bndbox/ymax').text)\n",
    "\n",
    "        row = [filename, blood_cells, xmin, xmax, ymin, ymax]\n",
    "        df.append(row)\n",
    "        cnt += 1\n",
    "\n",
    "data = pd.DataFrame(df, columns=['filename', 'cell_type', 'xmin', 'xmax', 'ymin', 'ymax'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['cell_type'].unique()\n",
    "labels_map = {}\n",
    "\n",
    "for num, label in enumerate(labels):\n",
    "    labels_map[label] = num+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "train_obj = []\n",
    "\n",
    "data['filename'] = data['filename'].apply(lambda x: x.replace('BCCD/Annotations/', '').replace('.jpg', ''))\n",
    "\n",
    "with open('BCCD/ImageSets/Main/train.txt', 'r') as train_file:\n",
    "    for line in train_file.readlines():\n",
    "        train_set.append(os.getcwd() + '/BCCD/JPEGImages/' + line.replace('\\n', '') + '.jpg')\n",
    "        \n",
    "        train_data = data.loc[data.filename == line.replace('\\n', '')]\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        difficulties = []\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        while i < train_data.shape[0]:\n",
    "            boxes.append([int(train_data.iloc[i]['xmin']), int(train_data.iloc[i]['xmax']), \n",
    "                         int(train_data.iloc[i]['ymin']), int(train_data.iloc[i]['ymax'])])\n",
    "            labels.append(labels_map[train_data.iloc[i]['cell_type']])\n",
    "            difficulties.append(0)\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        train_obj.append({\"boxes\": boxes, \"labels\": labels, \"difficulties\": difficulties})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = []\n",
    "val_obj = []\n",
    "\n",
    "with open('BCCD/ImageSets/Main/val.txt', 'r') as train_file:\n",
    "    for line in train_file.readlines():\n",
    "        val_set.append(os.getcwd() + '/BCCD/JPEGImages/' + line.replace('\\n', '') + '.jpg')\n",
    "        \n",
    "        val_data = data.loc[data.filename == line.replace('\\n', '')]\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        difficulties = []\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        while i < val_data.shape[0]:\n",
    "            boxes.append([int(val_data.iloc[i]['xmin']), int(val_data.iloc[i]['xmax']), \n",
    "                         int(val_data.iloc[i]['ymin']), int(val_data.iloc[i]['ymax'])])\n",
    "            labels.append(labels_map[val_data.iloc[i]['cell_type']])\n",
    "            difficulties.append(0)\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        val_obj.append({\"boxes\": boxes, \"labels\": labels, \"difficulties\": difficulties})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TRAIN_images.json', 'w') as train_file:\n",
    "    json.dump(train_set, train_file)\n",
    "\n",
    "with open('TRAIN_objects.json', 'w') as train_file:\n",
    "    json.dump(train_obj, train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TEST_images.json', 'w') as val_file:\n",
    "    json.dump(val_set, val_file)\n",
    "\n",
    "with open('TEST_objects.json', 'w') as val_file:\n",
    "    json.dump(val_obj, val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('label_map.json', 'w') as file:\n",
    "    json.dump(labels_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "              'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
    "label_map['background'] = 0\n",
    "rev_label_map = {v: k for k, v in label_map.items()}  # Inverse mapping\n",
    "\n",
    "distinct_colors = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8', '#f58231', '#911eb4', '#46f0f0', '#f032e6',\n",
    "                   '#d2f53c', '#fabebe', '#008080', '#000080', '#aa6e28', '#fffac8', '#800000', '#aaffc3', '#808000',\n",
    "                   '#ffd8b1', '#e6beff', '#808080', '#FFFFFF']\n",
    "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}\n",
    "\n",
    "\n",
    "def parse_annotation(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    for object in root.iter('object'):\n",
    "\n",
    "        difficult = int(object.find('difficult').text == '1')\n",
    "\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_map:\n",
    "            continue\n",
    "\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label_map[label])\n",
    "        difficulties.append(difficult)\n",
    "\n",
    "    return {'boxes': boxes, 'labels': labels, 'difficulties': difficulties}\n",
    "\n",
    "\n",
    "def create_data_lists(voc07_path, voc12_path, output_folder):\n",
    "    voc07_path = os.path.abspath(voc07_path)\n",
    "    voc12_path = os.path.abspath(voc12_path)\n",
    "\n",
    "    train_images = list()\n",
    "    train_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    for path in [voc07_path, voc12_path]:\n",
    "\n",
    "        with open(os.path.join(path, 'ImageSets/Main/trainval.txt')) as f:\n",
    "            ids = f.read().splitlines()\n",
    "\n",
    "        for id in ids:\n",
    "            objects = parse_annotation(os.path.join(path, 'Annotations', id + '.xml'))\n",
    "            if len(objects['boxes']) == 0:\n",
    "                continue\n",
    "            n_objects += len(objects)\n",
    "            train_objects.append(objects)\n",
    "            train_images.append(os.path.join(path, 'JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(train_objects) == len(train_images)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'TRAIN_images.json'), 'w') as j:\n",
    "        json.dump(train_images, j)\n",
    "    with open(os.path.join(output_folder, 'TRAIN_objects.json'), 'w') as j:\n",
    "        json.dump(train_objects, j)\n",
    "    with open(os.path.join(output_folder, 'label_map.json'), 'w') as j:\n",
    "        json.dump(label_map, j)\n",
    "\n",
    "    print('\\nThere are %d training images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(train_images), n_objects, os.path.abspath(output_folder)))\n",
    "\n",
    "    test_images = list()\n",
    "    test_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    with open(os.path.join(voc07_path, 'ImageSets/Main/test.txt')) as f:\n",
    "        ids = f.read().splitlines()\n",
    "\n",
    "    for id in ids:\n",
    "        objects = parse_annotation(os.path.join(voc07_path, 'Annotations', id + '.xml'))\n",
    "        if len(objects) == 0:\n",
    "            continue\n",
    "        test_objects.append(objects)\n",
    "        n_objects += len(objects)\n",
    "        test_images.append(os.path.join(voc07_path, 'JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(test_objects) == len(test_images)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'TEST_images.json'), 'w') as j:\n",
    "        json.dump(test_images, j)\n",
    "    with open(os.path.join(output_folder, 'TEST_objects.json'), 'w') as j:\n",
    "        json.dump(test_objects, j)\n",
    "\n",
    "    print('\\nThere are %d test images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(test_images), n_objects, os.path.abspath(output_folder)))\n",
    "\n",
    "\n",
    "def decimate(tensor, m):\n",
    "    assert tensor.dim() == len(m)\n",
    "    for d in range(tensor.dim()):\n",
    "        if m[d] is not None:\n",
    "            tensor = tensor.index_select(dim=d,\n",
    "                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties):\n",
    "    assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(\n",
    "        true_labels) == len(\n",
    "        true_difficulties)\n",
    "    n_classes = len(label_map)\n",
    "\n",
    "    true_images = list()\n",
    "    for i in range(len(true_labels)):\n",
    "        true_images.extend([i] * true_labels[i].size(0))\n",
    "    true_images = torch.LongTensor(true_images).to(\n",
    "        device)\n",
    "    true_boxes = torch.cat(true_boxes, dim=0)\n",
    "    true_labels = torch.cat(true_labels, dim=0)\n",
    "    true_difficulties = torch.cat(true_difficulties, dim=0)\n",
    "\n",
    "    assert true_images.size(0) == true_boxes.size(0) == true_labels.size(0)\n",
    "\n",
    "    det_images = list()\n",
    "    for i in range(len(det_labels)):\n",
    "        det_images.extend([i] * det_labels[i].size(0))\n",
    "    det_images = torch.LongTensor(det_images).to(device)\n",
    "    det_boxes = torch.cat(det_boxes, dim=0)\n",
    "    det_labels = torch.cat(det_labels, dim=0)\n",
    "    det_scores = torch.cat(det_scores, dim=0)\n",
    "\n",
    "    assert det_images.size(0) == det_boxes.size(0) == det_labels.size(0) == det_scores.size(0)\n",
    "\n",
    "    average_precisions = torch.zeros((n_classes - 1), dtype=torch.float)\n",
    "    for c in range(1, n_classes):\n",
    "        true_class_images = true_images[true_labels == c]\n",
    "        true_class_boxes = true_boxes[true_labels == c]\n",
    "        true_class_difficulties = true_difficulties[true_labels == c]\n",
    "        n_easy_class_objects = (1 - true_class_difficulties).sum().item()\n",
    "\n",
    "        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.uint8).to(device)\n",
    "\n",
    "        det_class_images = det_images[det_labels == c]\n",
    "        det_class_boxes = det_boxes[det_labels == c]\n",
    "        det_class_scores = det_scores[det_labels == c]\n",
    "        n_class_detections = det_class_boxes.size(0)\n",
    "        if n_class_detections == 0:\n",
    "            continue\n",
    "\n",
    "        det_class_scores, sort_ind = torch.sort(det_class_scores, dim=0, descending=True)\n",
    "        det_class_images = det_class_images[sort_ind]\n",
    "        det_class_boxes = det_class_boxes[sort_ind]\n",
    "\n",
    "        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)\n",
    "        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)\n",
    "        for d in range(n_class_detections):\n",
    "            this_detection_box = det_class_boxes[d].unsqueeze(0)\n",
    "            this_image = det_class_images[d]\n",
    "\n",
    "            object_boxes = true_class_boxes[true_class_images == this_image]\n",
    "            object_difficulties = true_class_difficulties[true_class_images == this_image]\n",
    "            if object_boxes.size(0) == 0:\n",
    "                false_positives[d] = 1\n",
    "                continue\n",
    "\n",
    "            overlaps = find_jaccard_overlap(this_detection_box, object_boxes)\n",
    "            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)\n",
    "\n",
    "            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n",
    "            if max_overlap.item() > 0.5:\n",
    "                if object_difficulties[ind] == 0:\n",
    "                    if true_class_boxes_detected[original_ind] == 0:\n",
    "                        true_positives[d] = 1\n",
    "                        true_class_boxes_detected[original_ind] = 1\n",
    "                    else:\n",
    "                        false_positives[d] = 1\n",
    "            else:\n",
    "                false_positives[d] = 1\n",
    "\n",
    "        cumul_true_positives = torch.cumsum(true_positives, dim=0)\n",
    "        cumul_false_positives = torch.cumsum(false_positives, dim=0)\n",
    "        cumul_precision = cumul_true_positives / (\n",
    "                cumul_true_positives + cumul_false_positives + 1e-10)\n",
    "        cumul_recall = cumul_true_positives / n_easy_class_objects\n",
    "\n",
    "        recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist()\n",
    "        precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)\n",
    "        for i, t in enumerate(recall_thresholds):\n",
    "            recalls_above_t = cumul_recall >= t\n",
    "            if recalls_above_t.any():\n",
    "                precisions[i] = cumul_precision[recalls_above_t].max()\n",
    "            else:\n",
    "                precisions[i] = 0.\n",
    "        average_precisions[c - 1] = precisions.mean()\n",
    "\n",
    "    mean_average_precision = average_precisions.mean().item()\n",
    "\n",
    "    average_precisions = {rev_label_map[c + 1]: v for c, v in enumerate(average_precisions.tolist())}\n",
    "\n",
    "    return average_precisions, mean_average_precision\n",
    "\n",
    "\n",
    "def xy_to_cxcy(xy):\n",
    "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n",
    "                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n",
    "\n",
    "\n",
    "def cxcy_to_xy(cxcy):\n",
    "    return torch.cat([cxcy[:, :2] - (cxcy[:, 2:] / 2),  # x_min, y_min\n",
    "                      cxcy[:, :2] + (cxcy[:, 2:] / 2)], 1)  # x_max, y_max\n",
    "\n",
    "\n",
    "def cxcy_to_gcxgcy(cxcy, priors_cxcy):\n",
    "    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10),  # g_c_x, g_c_y\n",
    "                      torch.log(cxcy[:, 2:] / priors_cxcy[:, 2:]) * 5], 1)  # g_w, g_h\n",
    "\n",
    "\n",
    "def gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n",
    "    return torch.cat([gcxgcy[:, :2] * priors_cxcy[:, 2:] / 10 + priors_cxcy[:, :2],  # c_x, c_y\n",
    "                      torch.exp(gcxgcy[:, 2:] / 5) * priors_cxcy[:, 2:]], 1)  # w, h\n",
    "\n",
    "\n",
    "def find_intersection(set_1, set_2):\n",
    "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
    "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
    "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
    "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n",
    "\n",
    "\n",
    "def find_jaccard_overlap(set_1, set_2):\n",
    "    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n",
    "\n",
    "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
    "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
    "\n",
    "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
    "\n",
    "    return intersection / union  # (n1, n2)\n",
    "\n",
    "\n",
    "def expand(image, boxes, filler):\n",
    "    original_h = image.size(1)\n",
    "    original_w = image.size(2)\n",
    "    max_scale = 4\n",
    "    scale = random.uniform(1, max_scale)\n",
    "    new_h = int(scale * original_h)\n",
    "    new_w = int(scale * original_w)\n",
    "\n",
    "    filler = torch.FloatTensor(filler)\n",
    "    new_image = torch.ones((3, new_h, new_w), dtype=torch.float) * filler.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "    left = random.randint(0, new_w - original_w)\n",
    "    right = left + original_w\n",
    "    top = random.randint(0, new_h - original_h)\n",
    "    bottom = top + original_h\n",
    "    new_image[:, top:bottom, left:right] = image\n",
    "\n",
    "    new_boxes = boxes + torch.FloatTensor([left, top, left, top]).unsqueeze(0)\n",
    "\n",
    "    return new_image, new_boxes\n",
    "\n",
    "\n",
    "def random_crop(image, boxes, labels, difficulties):\n",
    "    original_h = image.size(1)\n",
    "    original_w = image.size(2)\n",
    "    while True:\n",
    "        min_overlap = random.choice([0., .1, .3, .5, .7, .9, None])\n",
    "\n",
    "        if min_overlap is None:\n",
    "            return image, boxes, labels, difficulties\n",
    "\n",
    "        max_trials = 50\n",
    "        for _ in range(max_trials):\n",
    "            min_scale = 0.3\n",
    "            scale_h = random.uniform(min_scale, 1)\n",
    "            scale_w = random.uniform(min_scale, 1)\n",
    "            new_h = int(scale_h * original_h)\n",
    "            new_w = int(scale_w * original_w)\n",
    "\n",
    "            aspect_ratio = new_h / new_w\n",
    "            if not 0.5 < aspect_ratio < 2:\n",
    "                continue\n",
    "\n",
    "            left = random.randint(0, original_w - new_w)\n",
    "            right = left + new_w\n",
    "            top = random.randint(0, original_h - new_h)\n",
    "            bottom = top + new_h\n",
    "            crop = torch.FloatTensor([left, top, right, bottom])\n",
    "\n",
    "            overlap = find_jaccard_overlap(crop.unsqueeze(0),\n",
    "                                           boxes)\n",
    "            overlap = overlap.squeeze(0)\n",
    "\n",
    "            if overlap.max().item() < min_overlap:\n",
    "                continue\n",
    "\n",
    "            new_image = image[:, top:bottom, left:right]\n",
    "\n",
    "            bb_centers = (boxes[:, :2] + boxes[:, 2:]) / 2.\n",
    "\n",
    "            centers_in_crop = (bb_centers[:, 0] > left) * (bb_centers[:, 0] < right) * (bb_centers[:, 1] > top) * (\n",
    "                    bb_centers[:, 1] < bottom)\n",
    "\n",
    "            if not centers_in_crop.any():\n",
    "                continue\n",
    "\n",
    "            new_boxes = boxes[centers_in_crop, :]\n",
    "            new_labels = labels[centers_in_crop]\n",
    "            new_difficulties = difficulties[centers_in_crop]\n",
    "\n",
    "            new_boxes[:, :2] = torch.max(new_boxes[:, :2], crop[:2])\n",
    "            new_boxes[:, :2] -= crop[:2]\n",
    "            new_boxes[:, 2:] = torch.min(new_boxes[:, 2:], crop[2:])\n",
    "            new_boxes[:, 2:] -= crop[:2]\n",
    "\n",
    "            return new_image, new_boxes, new_labels, new_difficulties\n",
    "\n",
    "\n",
    "def flip(image, boxes):\n",
    "    new_image = FT.hflip(image)\n",
    "\n",
    "    new_boxes = boxes\n",
    "    new_boxes[:, 0] = image.width - boxes[:, 0] - 1\n",
    "    new_boxes[:, 2] = image.width - boxes[:, 2] - 1\n",
    "    new_boxes = new_boxes[:, [2, 1, 0, 3]]\n",
    "\n",
    "    return new_image, new_boxes\n",
    "\n",
    "\n",
    "def resize(image, boxes, dims=(300, 300), return_percent_coords=True):\n",
    "    new_image = FT.resize(image, dims)\n",
    "\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
    "    new_boxes = boxes / old_dims\n",
    "\n",
    "    if not return_percent_coords:\n",
    "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "        new_boxes = new_boxes * new_dims\n",
    "\n",
    "    return new_image, new_boxes\n",
    "\n",
    "\n",
    "def photometric_distort(image):\n",
    "    new_image = image\n",
    "\n",
    "    distortions = [FT.adjust_brightness,\n",
    "                   FT.adjust_contrast,\n",
    "                   FT.adjust_saturation,\n",
    "                   FT.adjust_hue]\n",
    "\n",
    "    random.shuffle(distortions)\n",
    "\n",
    "    for d in distortions:\n",
    "        if random.random() < 0.5:\n",
    "            if d.__name__ is 'adjust_hue':\n",
    "                adjust_factor = random.uniform(-18 / 255., 18 / 255.)\n",
    "            else:\n",
    "                adjust_factor = random.uniform(0.5, 1.5)\n",
    "\n",
    "            new_image = d(new_image, adjust_factor)\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def transform(image, boxes, labels, difficulties, split):\n",
    "    assert split in {'TRAIN', 'TEST'}\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    new_image = image\n",
    "    new_boxes = boxes\n",
    "    new_labels = labels\n",
    "    new_difficulties = difficulties\n",
    "    if split == 'TRAIN':\n",
    "        new_image = photometric_distort(new_image)\n",
    "\n",
    "        new_image = FT.to_tensor(new_image)\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            new_image, new_boxes = expand(new_image, boxes, filler=mean)\n",
    "\n",
    "        new_image, new_boxes, new_labels, new_difficulties = random_crop(new_image, new_boxes, new_labels,\n",
    "                                                                         new_difficulties)\n",
    "\n",
    "        new_image = FT.to_pil_image(new_image)\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            new_image, new_boxes = flip(new_image, new_boxes)\n",
    "\n",
    "    new_image, new_boxes = resize(new_image, new_boxes, dims=(300, 300))\n",
    "\n",
    "    new_image = FT.to_tensor(new_image)\n",
    "\n",
    "    new_image = FT.normalize(new_image, mean=mean, std=std)\n",
    "\n",
    "    return new_image, new_boxes, new_labels, new_difficulties\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, scale):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * scale\n",
    "    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[1]['lr'],))\n",
    "\n",
    "\n",
    "def accuracy(scores, targets, k):\n",
    "    batch_size = targets.size(0)\n",
    "    _, ind = scores.topk(k, 1, True, True)\n",
    "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "    correct_total = correct.view(-1).float().sum()\n",
    "    return correct_total.item() * (100.0 / batch_size)\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer):\n",
    "    state = {'epoch': epoch,\n",
    "             'model': model,\n",
    "             'optimizer': optimizer}\n",
    "    filename = 'checkpoint_ssd300.pth.tar'\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем модель для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalVOCDataset(Dataset):\n",
    "    def __init__(self, data_folder, split, keep_difficult=False):\n",
    "        self.split = split.upper()\n",
    "\n",
    "        assert self.split in {'TRAIN', 'TEST'}\n",
    "\n",
    "        self.data_folder = data_folder\n",
    "        self.keep_difficult = keep_difficult\n",
    "\n",
    "        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "\n",
    "        assert len(self.images) == len(self.objects)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        objects = self.objects[i]\n",
    "        boxes = torch.FloatTensor(objects['boxes'])\n",
    "        labels = torch.LongTensor(objects['labels'])\n",
    "        difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "\n",
    "        if not self.keep_difficult:\n",
    "            boxes = boxes[1 - difficulties]\n",
    "            labels = labels[1 - difficulties]\n",
    "            difficulties = difficulties[1 - difficulties]\n",
    "\n",
    "        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, split=self.split)\n",
    "\n",
    "        return image, boxes, labels, difficulties\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        images = list()\n",
    "        boxes = list()\n",
    "        labels = list()\n",
    "        difficulties = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "            labels.append(b[2])\n",
    "            difficulties.append(b[3])\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        return images, boxes, labels, difficulties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "\n",
    "        self.load_pretrained_layers()\n",
    "\n",
    "    def forward(self, image):\n",
    "        out = F.relu(self.conv1_1(image))\n",
    "        out = F.relu(self.conv1_2(out))\n",
    "        out = self.pool1(out)\n",
    "\n",
    "        out = F.relu(self.conv2_1(out))\n",
    "        out = F.relu(self.conv2_2(out))\n",
    "        out = self.pool2(out)\n",
    "\n",
    "        out = F.relu(self.conv3_1(out))\n",
    "        out = F.relu(self.conv3_2(out))\n",
    "        out = F.relu(self.conv3_3(out))\n",
    "        out = self.pool3(out)\n",
    "\n",
    "        out = F.relu(self.conv4_1(out))\n",
    "        out = F.relu(self.conv4_2(out))\n",
    "        out = F.relu(self.conv4_3(out))\n",
    "        conv4_3_feats = out\n",
    "        out = self.pool4(out)\n",
    "\n",
    "        out = F.relu(self.conv5_1(out))\n",
    "        out = F.relu(self.conv5_2(out))\n",
    "        out = F.relu(self.conv5_3(out))\n",
    "        out = self.pool5(out)\n",
    "\n",
    "        out = F.relu(self.conv6(out))\n",
    "\n",
    "        conv7_feats = F.relu(self.conv7(out))\n",
    "\n",
    "        return conv4_3_feats, conv7_feats\n",
    "\n",
    "    def load_pretrained_layers(self):\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "\n",
    "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "\n",
    "        for i, param in enumerate(param_names[:-4]):\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])\n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])\n",
    "\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "        print(\"\\nLoaded base model.\\n\")\n",
    "\n",
    "\n",
    "class AuxiliaryConvolutions(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions, self).__init__()\n",
    "\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
    "\n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
    "\n",
    "        self.init_conv2d()\n",
    "\n",
    "    def init_conv2d(self):\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "    def forward(self, conv7_feats):\n",
    "        out = F.relu(self.conv8_1(conv7_feats))  # (N, 256, 19, 19)\n",
    "        out = F.relu(self.conv8_2(out))  # (N, 512, 10, 10)\n",
    "        conv8_2_feats = out  # (N, 512, 10, 10)\n",
    "\n",
    "        out = F.relu(self.conv9_1(out))  # (N, 128, 10, 10)\n",
    "        out = F.relu(self.conv9_2(out))  # (N, 256, 5, 5)\n",
    "        conv9_2_feats = out  # (N, 256, 5, 5)\n",
    "\n",
    "        out = F.relu(self.conv10_1(out))  # (N, 128, 5, 5)\n",
    "        out = F.relu(self.conv10_2(out))  # (N, 256, 3, 3)\n",
    "        conv10_2_feats = out  # (N, 256, 3, 3)\n",
    "\n",
    "        out = F.relu(self.conv11_1(out))  # (N, 128, 3, 3)\n",
    "        conv11_2_feats = F.relu(self.conv11_2(out))  # (N, 256, 1, 1)\n",
    "\n",
    "        # Higher-level feature maps\n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n",
    "\n",
    "\n",
    "class PredictionConvolutions(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        n_boxes = {'conv4_3': 4,\n",
    "                   'conv7': 6,\n",
    "                   'conv8_2': 6,\n",
    "                   'conv9_2': 6,\n",
    "                   'conv10_2': 4,\n",
    "                   'conv11_2': 4}\n",
    "        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)\n",
    "\n",
    "        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
    "\n",
    "        self.init_conv2d()\n",
    "\n",
    "    def init_conv2d(self):\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
    "        batch_size = conv4_3_feats.size(0)\n",
    "\n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats)\n",
    "        l_conv4_3 = l_conv4_3.permute(0, 2, 3, 1).contiguous()\n",
    "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)\n",
    "\n",
    "        l_conv7 = self.loc_conv7(conv7_feats)\n",
    "        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()\n",
    "        l_conv7 = l_conv7.view(batch_size, -1, 4)\n",
    "\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)\n",
    "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)\n",
    "\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)\n",
    "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)\n",
    "\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats)\n",
    "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)\n",
    "\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats)\n",
    "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)\n",
    "\n",
    "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)\n",
    "        c_conv4_3 = c_conv4_3.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv4_3 = c_conv4_3.view(batch_size, -1, self.n_classes)\n",
    "\n",
    "        c_conv7 = self.cl_conv7(conv7_feats)\n",
    "        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv7 = c_conv7.view(batch_size, -1, self.n_classes)\n",
    "\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)\n",
    "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)\n",
    "\n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)\n",
    "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)\n",
    "\n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)\n",
    "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)\n",
    "\n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)\n",
    "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)\n",
    "\n",
    "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)  # (N, 8732, 4)\n",
    "        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2],\n",
    "                                   dim=1)\n",
    "\n",
    "        return locs, classes_scores\n",
    "\n",
    "\n",
    "class SSD300(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SSD300, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.base = VGGBase()\n",
    "        self.aux_convs = AuxiliaryConvolutions()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "\n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))\n",
    "        nn.init.constant_(self.rescale_factors, 20)\n",
    "\n",
    "        self.priors_cxcy = self.create_prior_boxes()\n",
    "\n",
    "    def forward(self, image):\n",
    "        conv4_3_feats, conv7_feats = self.base(image)\n",
    "\n",
    "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()\n",
    "        conv4_3_feats = conv4_3_feats / norm\n",
    "        conv4_3_feats = conv4_3_feats * self.rescale_factors\n",
    "\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = self.aux_convs(conv7_feats)\n",
    "\n",
    "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats,\n",
    "                                               conv11_2_feats)\n",
    "\n",
    "        return locs, classes_scores\n",
    "\n",
    "    def create_prior_boxes(self):\n",
    "        fmap_dims = {'conv4_3': 38,\n",
    "                     'conv7': 19,\n",
    "                     'conv8_2': 10,\n",
    "                     'conv9_2': 5,\n",
    "                     'conv10_2': 3,\n",
    "                     'conv11_2': 1}\n",
    "\n",
    "        obj_scales = {'conv4_3': 0.1,\n",
    "                      'conv7': 0.2,\n",
    "                      'conv8_2': 0.375,\n",
    "                      'conv9_2': 0.55,\n",
    "                      'conv10_2': 0.725,\n",
    "                      'conv11_2': 0.9}\n",
    "\n",
    "        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
    "                         'conv7': [1., 2., 3., 0.5, .333],\n",
    "                         'conv8_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv9_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv10_2': [1., 2., 0.5],\n",
    "                         'conv11_2': [1., 2., 0.5]}\n",
    "\n",
    "        fmaps = list(fmap_dims.keys())\n",
    "\n",
    "        prior_boxes = []\n",
    "\n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmap_dims[fmap]):\n",
    "                for j in range(fmap_dims[fmap]):\n",
    "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
    "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
    "\n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
    "                        if ratio == 1.:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
    "                            except IndexError:\n",
    "                                additional_scale = 1.\n",
    "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "\n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)\n",
    "        prior_boxes.clamp_(0, 1)\n",
    "\n",
    "        return prior_boxes\n",
    "\n",
    "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors = self.priors_cxcy.size(0)\n",
    "        predicted_scores = F.softmax(predicted_scores, dim=2)\n",
    "\n",
    "        all_images_boxes = list()\n",
    "        all_images_labels = list()\n",
    "        all_images_scores = list()\n",
    "\n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            decoded_locs = cxcy_to_xy(\n",
    "                gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))\n",
    "\n",
    "            image_boxes = list()\n",
    "            image_labels = list()\n",
    "            image_scores = list()\n",
    "\n",
    "            max_scores, best_label = predicted_scores[i].max(dim=1)\n",
    "\n",
    "            for c in range(1, self.n_classes):\n",
    "                class_scores = predicted_scores[i][:, c]\n",
    "                score_above_min_score = class_scores > min_score\n",
    "                n_above_min_score = score_above_min_score.sum().item()\n",
    "                if n_above_min_score == 0:\n",
    "                    continue\n",
    "                class_scores = class_scores[score_above_min_score]\n",
    "                class_decoded_locs = decoded_locs[score_above_min_score]\n",
    "\n",
    "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)\n",
    "                class_decoded_locs = class_decoded_locs[sort_ind]\n",
    "\n",
    "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)\n",
    "\n",
    "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)\n",
    "\n",
    "                for box in range(class_decoded_locs.size(0)):\n",
    "                    if suppress[box] == 1:\n",
    "                        continue\n",
    "\n",
    "                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n",
    "\n",
    "                    suppress[box] = 0\n",
    "\n",
    "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
    "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
    "                image_scores.append(class_scores[1 - suppress])\n",
    "\n",
    "            if len(image_boxes) == 0:\n",
    "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
    "                image_labels.append(torch.LongTensor([0]).to(device))\n",
    "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
    "\n",
    "            image_boxes = torch.cat(image_boxes, dim=0)\n",
    "            image_labels = torch.cat(image_labels, dim=0)\n",
    "            image_scores = torch.cat(image_scores, dim=0)\n",
    "            n_objects = image_scores.size(0)\n",
    "\n",
    "            if n_objects > top_k:\n",
    "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
    "                image_scores = image_scores[:top_k]\n",
    "                image_boxes = image_boxes[sort_ind][:top_k]\n",
    "                image_labels = image_labels[sort_ind][:top_k]\n",
    "\n",
    "            all_images_boxes.append(image_boxes)\n",
    "            all_images_labels.append(image_labels)\n",
    "            all_images_scores.append(image_scores)\n",
    "\n",
    "        return all_images_boxes, all_images_labels, all_images_scores\n",
    "\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.priors_cxcy = priors_cxcy\n",
    "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.smooth_l1 = nn.L1Loss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
    "\n",
    "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors = self.priors_cxcy.size(0)\n",
    "        n_classes = predicted_scores.size(2)\n",
    "\n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "\n",
    "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)\n",
    "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            n_objects = boxes[i].size(0)\n",
    "\n",
    "            overlap = find_jaccard_overlap(boxes[i], self.priors_xy)\n",
    "\n",
    "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)\n",
    "\n",
    "            _, prior_for_each_object = overlap.max(dim=1)\n",
    "\n",
    "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
    "\n",
    "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
    "\n",
    "            label_for_each_prior = labels[i][object_for_each_prior]\n",
    "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0\n",
    "\n",
    "            true_classes[i] = label_for_each_prior\n",
    "\n",
    "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)\n",
    "\n",
    "        positive_priors = true_classes != 0\n",
    "\n",
    "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])\n",
    "\n",
    "        n_positives = positive_priors.sum(dim=1)\n",
    "        n_hard_negatives = self.neg_pos_ratio * n_positives\n",
    "\n",
    "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))\n",
    "        conf_loss_all = conf_loss_all.view(batch_size, n_priors)\n",
    "\n",
    "        conf_loss_pos = conf_loss_all[positive_priors]\n",
    "\n",
    "        conf_loss_neg = conf_loss_all.clone()\n",
    "        conf_loss_neg[positive_priors] = 0.\n",
    "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)\n",
    "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)\n",
    "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]\n",
    "\n",
    "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()\n",
    "\n",
    "        return conf_loss + self.alpha * loc_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './'\n",
    "keep_difficult = True\n",
    "\n",
    "n_classes = len(label_map)\n",
    "\n",
    "checkpoint = None\n",
    "batch_size = 8\n",
    "iterations = 120000\n",
    "workers = 0\n",
    "print_freq = 200\n",
    "lr = 1e-3  #\n",
    "decay_lr_at = [80000, 100000]\n",
    "decay_lr_to = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "grad_clip = None\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    global start_epoch, label_map, epoch, checkpoint, decay_lr_at\n",
    "\n",
    "    if checkpoint is None:\n",
    "        start_epoch = 0\n",
    "        model = SSD300(n_classes=n_classes)\n",
    "        biases = list()\n",
    "        not_biases = list()\n",
    "        for param_name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if param_name.endswith('.bias'):\n",
    "                    biases.append(param)\n",
    "                else:\n",
    "                    not_biases.append(param)\n",
    "        optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
    "                                    lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print('\\nLoaded checkpoint from epoch %d.\\n' % start_epoch)\n",
    "        model = checkpoint['model']\n",
    "        optimizer = checkpoint['optimizer']\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n",
    "\n",
    "    train_dataset = PascalVOCDataset(data_folder,\n",
    "                                     split='train',\n",
    "                                     keep_difficult=keep_difficult)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
    "                                               pin_memory=True)\n",
    "\n",
    "    epochs = iterations // (len(train_dataset) // 32)\n",
    "    decay_lr_at = [it // (len(train_dataset) // 32) for it in decay_lr_at]\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        if epoch in decay_lr_at:\n",
    "            adjust_learning_rate(optimizer, decay_lr_to)\n",
    "\n",
    "        train(train_loader=train_loader,\n",
    "              model=model,\n",
    "              criterion=criterion,\n",
    "              optimizer=optimizer,\n",
    "              epoch=epoch)\n",
    "\n",
    "        save_checkpoint(epoch, model, optimizer)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        images = images.to(device)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "\n",
    "        predicted_locs, predicted_scores = model(images)\n",
    "\n",
    "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                  batch_time=batch_time,\n",
    "                                                                  data_time=data_time, loss=losses))\n",
    "    del predicted_locs, predicted_scores, images, boxes, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
