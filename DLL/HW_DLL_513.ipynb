{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75102e94",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "Обучите нейронную сеть решать шифр Цезаря.\n",
    "\n",
    "Что необходимо сделать:\n",
    "\n",
    "Написать алгоритм шифра Цезаря для генерации выборки (сдвиг на К каждой буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и тп)\n",
    "Сделать нейронную сеть\n",
    "Обучить ее (вход - зашифрованная фраза, выход - дешифрованная фраза)\n",
    "Проверить качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc84c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import time\n",
    "\n",
    "key = 10\n",
    "vocab = [char for char in 'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ']\n",
    "\n",
    "def encrypt(text):\n",
    "    indexes = [vocab.index(char) for char in text]\n",
    "    encrypted_indexes = [(idx + key) % len(vocab) for idx in indexes]\n",
    "    encrypted_chars = [vocab[idx] for idx in encrypted_indexes]\n",
    "    encrypted = ''.join(encrypted_chars)\n",
    "    return encrypted\n",
    "\n",
    "num_examples = 128\n",
    "message_length = 32\n",
    "\n",
    "def dataset(num_examples):\n",
    "    dataset = []\n",
    "    for x in range(num_examples):\n",
    "        ex_out = ''.join([random.choice(vocab) for x in range(message_length)])\n",
    "        ex_in = encrypt(''.join(ex_out))\n",
    "        ex_in = [vocab.index(x) for x in ex_in]\n",
    "        ex_out = [vocab.index(x) for x in ex_out]\n",
    "        dataset.append([torch.tensor(ex_in), torch.tensor(ex_out)])\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ac2863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.73%\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 10\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "softmax = torch.nn.functional.softmax\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(embed.parameters()) +\n",
    "                             list(lstm.parameters()) +\n",
    "                             list(linear.parameters()), lr=0.001)\n",
    "\n",
    "def zero_hidden():\n",
    "    return (torch.zeros(1, 1, hidden_dim),\n",
    "            torch.zeros(1, 1, hidden_dim))\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "accuracies, max_accuracy = [], 0\n",
    "for x in range(num_epochs):\n",
    "    for encrypted, original in dataset(num_examples):\n",
    "        lstm_in = embed(encrypted)\n",
    "        lstm_in = lstm_in.unsqueeze(1)\n",
    "        lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden())\n",
    "        scores = linear(lstm_out)\n",
    "        scores = scores.transpose(1, 2)\n",
    "        original = original.unsqueeze(1)\n",
    "        loss = loss_fn(scores, original) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    matches, total = 0, 0\n",
    "    for encrypted, original in dataset(num_examples):\n",
    "        lstm_in = embed(encrypted)\n",
    "        lstm_in = lstm_in.unsqueeze(1)\n",
    "        lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden())\n",
    "        scores = linear(lstm_out)\n",
    "        predictions = softmax(scores, dim=2)\n",
    "        _, batch_out = predictions.max(dim=2)\n",
    "        batch_out = batch_out.squeeze(1)\n",
    "        matches += torch.eq(batch_out, original).sum().item()\n",
    "        total += torch.numel(batch_out)\n",
    "    accuracy = matches / total\n",
    "    print('Accuracy: {:4.2f}%'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c041fa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 0.092, Train loss: 3.199\n",
      "Epoch 1. Time: 0.070, Train loss: 2.515\n",
      "Epoch 2. Time: 0.070, Train loss: 1.941\n",
      "Epoch 3. Time: 0.073, Train loss: 1.479\n",
      "Epoch 4. Time: 0.084, Train loss: 1.154\n",
      "Epoch 5. Time: 0.076, Train loss: 0.921\n",
      "Epoch 6. Time: 0.072, Train loss: 0.727\n",
      "Epoch 7. Time: 0.076, Train loss: 0.596\n",
      "Epoch 8. Time: 0.070, Train loss: 0.500\n",
      "Epoch 9. Time: 0.071, Train loss: 0.412\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 10\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, sentences, state=None):\n",
    "        embed = self.embed(sentences)\n",
    "        o, s = self.rnn(embed)\n",
    "        out = self.linear(o)\n",
    "        return out\n",
    "\n",
    "model = Network()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.05)\n",
    "\n",
    "for ep in range(10):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "    for encrypted, original in dataset(num_examples):\n",
    "        optimizer.zero_grad()\n",
    "        answers = model.forward(encrypted.unsqueeze(1))\n",
    "        answers = answers.view(-1, vocab_size)\n",
    "        loss = criterion(answers, original)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
    "\n",
    "with torch.no_grad():\n",
    "    matches, total = 0, 0\n",
    "    for encrypted, original in dataset(num_examples):\n",
    "        answers = model.forward(encrypted.unsqueeze(1))\n",
    "        predictions = torch.nn.functional.softmax(answers, dim=2)\n",
    "        _, batch_out = predictions.max(dim=2)\n",
    "        batch_out = batch_out.squeeze(1)\n",
    "        matches += torch.eq(batch_out, original).sum().item()\n",
    "        total += torch.numel(batch_out)\n",
    "    accuracy = matches / total\n",
    "    print('Accuracy: {:4.2f}%'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1fc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36686a14",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "Выполнить практическую работу из лекционного ноутбука.\n",
    "\n",
    "Построить RNN-ячейку на основе полносвязных слоев\n",
    "Применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7c6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from string import ascii_lowercase\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2661feec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10368</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>Lisa Simpson: Maggie, look. What's that?</td>\n",
       "      <td>235000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>Maggie, look. What's that?</td>\n",
       "      <td>maggie look whats that</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10369</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>Lisa Simpson: Lee-mur. Lee-mur.</td>\n",
       "      <td>237000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>Lee-mur. Lee-mur.</td>\n",
       "      <td>lee-mur lee-mur</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10370</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>Lisa Simpson: Zee-boo. Zee-boo.</td>\n",
       "      <td>239000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>Zee-boo. Zee-boo.</td>\n",
       "      <td>zee-boo zee-boo</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10372</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "      <td>Lisa Simpson: I'm trying to teach Maggie that ...</td>\n",
       "      <td>245000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>I'm trying to teach Maggie that nature doesn't...</td>\n",
       "      <td>im trying to teach maggie that nature doesnt e...</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10374</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>Lisa Simpson: It's like an ox, only it has a h...</td>\n",
       "      <td>254000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>It's like an ox, only it has a hump and a dewl...</td>\n",
       "      <td>its like an ox only it has a hump and a dewlap...</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id  episode_id  number  \\\n",
       "0           0  10368          35      29   \n",
       "1           1  10369          35      30   \n",
       "2           2  10370          35      31   \n",
       "3           3  10372          35      33   \n",
       "4           4  10374          35      35   \n",
       "\n",
       "                                            raw_text  timestamp_in_ms  \\\n",
       "0           Lisa Simpson: Maggie, look. What's that?           235000   \n",
       "1                    Lisa Simpson: Lee-mur. Lee-mur.           237000   \n",
       "2                    Lisa Simpson: Zee-boo. Zee-boo.           239000   \n",
       "3  Lisa Simpson: I'm trying to teach Maggie that ...           245000   \n",
       "4  Lisa Simpson: It's like an ox, only it has a h...           254000   \n",
       "\n",
       "   speaking_line  character_id  location_id raw_character_text  \\\n",
       "0           True             9          5.0       Lisa Simpson   \n",
       "1           True             9          5.0       Lisa Simpson   \n",
       "2           True             9          5.0       Lisa Simpson   \n",
       "3           True             9          5.0       Lisa Simpson   \n",
       "4           True             9          5.0       Lisa Simpson   \n",
       "\n",
       "  raw_location_text                                       spoken_words  \\\n",
       "0      Simpson Home                         Maggie, look. What's that?   \n",
       "1      Simpson Home                                  Lee-mur. Lee-mur.   \n",
       "2      Simpson Home                                  Zee-boo. Zee-boo.   \n",
       "3      Simpson Home  I'm trying to teach Maggie that nature doesn't...   \n",
       "4      Simpson Home  It's like an ox, only it has a hump and a dewl...   \n",
       "\n",
       "                                     normalized_text  word_count  \n",
       "0                             maggie look whats that         4.0  \n",
       "1                                    lee-mur lee-mur         2.0  \n",
       "2                                    zee-boo zee-boo         2.0  \n",
       "3  im trying to teach maggie that nature doesnt e...        24.0  \n",
       "4  its like an ox only it has a hump and a dewlap...        18.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b0ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = df['normalized_text'].tolist()\n",
    "text = [[char for char in sent] for sent in sents if isinstance(sent, str)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6c23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_TO_INDEX = {w: i for i, w in enumerate(ascii_lowercase, 1)}\n",
    "CHAR_TO_INDEX[' '] = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a022550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.36635754292535"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(sent) for sent in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c256b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13,  1,  7,  7,  9,  5, 27, 12, 15, 15, 11, 27, 23,  8,  1, 20, 19, 27,\n",
       "         20,  8,  1, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [12,  5,  5,  0, 13, 21, 18, 27, 12,  5,  5,  0, 13, 21, 18,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [26,  5,  5,  0,  2, 15, 15, 27, 26,  5,  5,  0,  2, 15, 15,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 9, 13, 27, 20, 18, 25,  9, 14,  7, 27, 20, 15, 27, 20,  5,  1,  3,  8,\n",
       "         27, 13,  1,  7,  7,  9,  5, 27, 20,  8,  1, 20, 27, 14,  1, 20, 21, 18,\n",
       "          5, 27,  4, 15,  5, 19, 14, 20, 27,  5, 14,  4, 27, 23],\n",
       "        [ 9, 20, 19, 27, 12,  9, 11,  5, 27,  1, 14, 27, 15, 24, 27, 15, 14, 12,\n",
       "         25, 27,  9, 20, 27,  8,  1, 19, 27,  1, 27,  8, 21, 13, 16, 27,  1, 14,\n",
       "          4, 27,  1, 27,  4,  5, 23, 12,  1, 16, 27,  8, 21, 13],\n",
       "        [25, 15, 21, 27, 11, 14, 15, 23, 27,  8,  9, 19, 27,  2, 12, 15, 15,  4,\n",
       "         27, 20, 25, 16,  5, 27,  8, 15, 23, 27, 18, 15, 13,  1, 14, 20,  9,  3,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [15,  8, 27, 25,  5,  1,  8, 27, 23,  8,  1, 20, 19, 27, 13, 25, 27, 19,\n",
       "          8, 15,  5, 27, 19,  9, 26,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [18,  9, 14,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [25,  5, 19, 27,  4,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [15, 15,  8, 27, 12, 15, 15, 11, 27, 13,  1,  7,  7,  9,  5, 27, 23,  8,\n",
       "          1, 20, 27,  9, 19, 27, 20,  8,  1, 20, 27,  4, 15,  0,  4,  5,  3,  0,\n",
       "          1,  8,  0,  5,  4, 18, 15, 14, 27,  4, 15,  4,  5,  3]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "\n",
    "X = torch.zeros((len(text), MAX_LEN), dtype=int)\n",
    "for i in range(len(text)):\n",
    "    for j, w in enumerate(text[i]):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        if w.lower() in CHAR_TO_INDEX:\n",
    "            X[i][j] = CHAR_TO_INDEX[w.lower()]\n",
    "\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5d75f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13,  1,  7,  7,  9,  5, 27, 12, 15, 15, 11, 27, 23,  8,  1, 20, 19, 27,\n",
       "         20,  8,  1, 20,  0,  0,  0],\n",
       "        [12,  5,  5,  0, 13, 21, 18, 27, 12,  5,  5,  0, 13, 21, 18,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0],\n",
       "        [26,  5,  5,  0,  2, 15, 15, 27, 26,  5,  5,  0,  2, 15, 15,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 9, 13, 27, 20, 18, 25,  9, 14,  7, 27, 20, 15, 27, 20,  5,  1,  3,  8,\n",
       "         27, 13,  1,  7,  7,  9,  5],\n",
       "        [ 9, 20, 19, 27, 12,  9, 11,  5, 27,  1, 14, 27, 15, 24, 27, 15, 14, 12,\n",
       "         25, 27,  9, 20, 27,  8,  1],\n",
       "        [25, 15, 21, 27, 11, 14, 15, 23, 27,  8,  9, 19, 27,  2, 12, 15, 15,  4,\n",
       "         27, 20, 25, 16,  5, 27,  8],\n",
       "        [15,  8, 27, 25,  5,  1,  8, 27, 23,  8,  1, 20, 19, 27, 13, 25, 27, 19,\n",
       "          8, 15,  5, 27, 19,  9, 26],\n",
       "        [18,  9, 14,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0],\n",
       "        [25,  5, 19, 27,  4,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0],\n",
       "        [15, 15,  8, 27, 12, 15, 15, 11, 27, 13,  1,  7,  7,  9,  5, 27, 23,  8,\n",
       "          1, 20, 27,  9, 19, 27, 20]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 25\n",
    "\n",
    "y = torch.zeros((len(text), MAX_LEN), dtype=int)\n",
    "for i in range(len(text)):\n",
    "    for j, w in enumerate(text[i]):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        if w.lower() in CHAR_TO_INDEX:\n",
    "            y[i][j] = CHAR_TO_INDEX[w.lower()]\n",
    "\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4270a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e54bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationNetwork(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(TextGenerationNetwork, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dense = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, train=True, state=None):\n",
    "        with torch.set_grad_enabled(train):\n",
    "            out = self.embedding(x)\n",
    "            out, state = self.rnn(out)\n",
    "            out = self.dense(out)\n",
    "            out = out.squeeze(0)\n",
    "        return out, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "965ecd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112513371b75425f92006ad70861b99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, time: 1.8s, train loss: 2.184\n",
      "hello t t t t t t t t t t\n",
      "\n",
      "Epoch: 2, time: 1.8s, train loss: 1.878\n",
      "hello t t t t t t t t t t\n",
      "\n",
      "Epoch: 3, time: 1.7s, train loss: 1.803\n",
      "hellou b b b b b b b b b \n",
      "\n",
      "Epoch: 4, time: 1.8s, train loss: 1.755\n",
      "hellout b b b b b b b b b\n",
      "\n",
      "Epoch: 5, time: 1.8s, train loss: 1.719\n",
      "hellouthe b b b b b b b b\n",
      "\n",
      "Epoch: 6, time: 1.7s, train loss: 1.691\n",
      "hellouthe b b b b b b b b\n",
      "\n",
      "Epoch: 7, time: 1.7s, train loss: 1.667\n",
      "hellouthe b b b b b b b b\n",
      "\n",
      "Epoch: 8, time: 1.7s, train loss: 1.648\n",
      "hellouthe b b b b b b b b\n",
      "\n",
      "Epoch: 9, time: 1.7s, train loss: 1.631\n",
      "hellouthe b b b b b b b b\n",
      "\n",
      "Epoch: 10, time: 1.7s, train loss: 1.617\n",
      "hellouthe b b b b b b b b\n",
      "\n",
      "Epoch: 11, time: 1.7s, train loss: 1.604\n",
      "hellouthe he he he he he \n",
      "\n",
      "Epoch: 12, time: 1.7s, train loss: 1.592\n",
      "hellouthe he he he he he \n",
      "\n",
      "Epoch: 13, time: 1.7s, train loss: 1.581\n",
      "hellouthe he he he he he \n",
      "\n",
      "Epoch: 14, time: 1.7s, train loss: 1.571\n",
      "hellouthe he he he he he \n",
      "\n",
      "Epoch: 15, time: 1.7s, train loss: 1.562\n",
      "hellouthe he he he he he \n",
      "\n",
      "Epoch: 16, time: 1.7s, train loss: 1.553\n",
      "helloutherererererererere\n",
      "\n",
      "Epoch: 17, time: 1.7s, train loss: 1.544\n",
      "helloutherererererererere\n",
      "\n",
      "Epoch: 18, time: 1.7s, train loss: 1.536\n",
      "helloutherererererererere\n",
      "\n",
      "Epoch: 19, time: 1.7s, train loss: 1.528\n",
      "helloutherererererererere\n",
      "\n",
      "Epoch: 20, time: 1.7s, train loss: 1.521\n",
      "helloutherererererererere\n",
      "\n",
      "Epoch: 21, time: 1.7s, train loss: 1.514\n",
      "helloutherererererererere\n",
      "\n",
      "Epoch: 22, time: 1.7s, train loss: 1.507\n",
      "helloutherererererererere\n",
      "\n",
      "Epoch: 23, time: 1.7s, train loss: 1.501\n",
      "helloutherererererererere\n",
      "\n",
      "Epoch: 24, time: 1.8s, train loss: 1.495\n",
      "helloutherererererererere\n",
      "\n",
      "Epoch: 25, time: 1.7s, train loss: 1.489\n",
      "helloutherererererererere\n",
      "\n",
      "Epoch: 26, time: 1.7s, train loss: 1.483\n",
      "helloutherererererererere\n",
      "\n",
      "Epoch: 27, time: 1.7s, train loss: 1.477\n",
      "helloutheroutheroutherout\n",
      "\n",
      "Epoch: 28, time: 1.7s, train loss: 1.472\n",
      "helloutheroutheroutherout\n",
      "\n",
      "Epoch: 29, time: 1.7s, train loss: 1.466\n",
      "helloutheroutheroutherout\n",
      "\n",
      "Epoch: 30, time: 1.7s, train loss: 1.461\n",
      "helloutheroutheroutherout\n",
      "\n",
      "Epoch: 31, time: 1.7s, train loss: 1.456\n",
      "helloutheroutheroutherout\n",
      "\n",
      "Epoch: 32, time: 1.7s, train loss: 1.451\n",
      "hellowherowherowherowhero\n",
      "\n",
      "Epoch: 33, time: 1.7s, train loss: 1.447\n",
      "helloheroheroheroherohero\n",
      "\n",
      "Epoch: 34, time: 1.7s, train loss: 1.442\n",
      "helloheroheroheroherohero\n",
      "\n",
      "Epoch: 35, time: 1.7s, train loss: 1.437\n",
      "helloheroheroheroherohero\n",
      "\n",
      "Epoch: 36, time: 1.7s, train loss: 1.433\n",
      "helloheroheroheroherohero\n",
      "\n",
      "Epoch: 37, time: 1.7s, train loss: 1.428\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 38, time: 1.7s, train loss: 1.424\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 39, time: 1.7s, train loss: 1.420\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 40, time: 1.7s, train loss: 1.416\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 41, time: 1.7s, train loss: 1.411\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 42, time: 1.7s, train loss: 1.407\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 43, time: 1.7s, train loss: 1.403\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 44, time: 1.7s, train loss: 1.400\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 45, time: 1.7s, train loss: 1.396\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 46, time: 1.7s, train loss: 1.392\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 47, time: 1.7s, train loss: 1.388\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 48, time: 1.7s, train loss: 1.385\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 49, time: 1.7s, train loss: 1.381\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 50, time: 1.7s, train loss: 1.378\n",
      "hellohererererererererere\n",
      "\n",
      "Epoch: 51, time: 1.7s, train loss: 1.374\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 52, time: 1.7s, train loss: 1.371\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 53, time: 1.7s, train loss: 1.368\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 54, time: 1.7s, train loss: 1.364\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 55, time: 1.7s, train loss: 1.361\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 56, time: 1.8s, train loss: 1.358\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 57, time: 1.8s, train loss: 1.355\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 58, time: 1.7s, train loss: 1.352\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 59, time: 1.8s, train loss: 1.349\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 60, time: 1.7s, train loss: 1.346\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 61, time: 1.7s, train loss: 1.343\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 62, time: 1.8s, train loss: 1.341\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 63, time: 1.7s, train loss: 1.338\n",
      "helloheanoheanoheanoheano\n",
      "\n",
      "Epoch: 64, time: 2.0s, train loss: 1.335\n",
      "helloheaheaheaheaheaheahe\n",
      "\n",
      "Epoch: 65, time: 2.0s, train loss: 1.333\n",
      "helloheaheaheaheaheaheahe\n",
      "\n",
      "Epoch: 66, time: 1.9s, train loss: 1.330\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 67, time: 1.8s, train loss: 1.327\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 68, time: 2.1s, train loss: 1.325\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 69, time: 1.8s, train loss: 1.322\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 70, time: 1.9s, train loss: 1.320\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 71, time: 1.8s, train loss: 1.317\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 72, time: 1.8s, train loss: 1.315\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 73, time: 1.8s, train loss: 1.313\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 74, time: 1.8s, train loss: 1.310\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 75, time: 1.8s, train loss: 1.308\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 76, time: 1.7s, train loss: 1.306\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 77, time: 1.7s, train loss: 1.304\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 78, time: 1.7s, train loss: 1.301\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 79, time: 1.7s, train loss: 1.299\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 80, time: 1.7s, train loss: 1.297\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 81, time: 1.7s, train loss: 1.295\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 82, time: 1.7s, train loss: 1.293\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 83, time: 1.7s, train loss: 1.291\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 84, time: 1.7s, train loss: 1.289\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 85, time: 1.7s, train loss: 1.287\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 86, time: 1.7s, train loss: 1.285\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 87, time: 1.7s, train loss: 1.283\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 88, time: 1.7s, train loss: 1.281\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 89, time: 1.7s, train loss: 1.279\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 90, time: 1.7s, train loss: 1.277\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 91, time: 1.7s, train loss: 1.275\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 92, time: 1.7s, train loss: 1.274\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 93, time: 1.8s, train loss: 1.272\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 94, time: 1.7s, train loss: 1.270\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 95, time: 1.7s, train loss: 1.268\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 96, time: 1.7s, train loss: 1.266\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 97, time: 1.7s, train loss: 1.265\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 98, time: 1.7s, train loss: 1.263\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 99, time: 1.7s, train loss: 1.261\n",
      "helloheveveveveveveveveve\n",
      "\n",
      "Epoch: 100, time: 1.7s, train loss: 1.260\n",
      "helloheveveveveveveveveve\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    sent = 'hello'\n",
    "    id = -1\n",
    "    while id != 0 and len(sent) < MAX_LEN:\n",
    "        sent_as_tensor = [torch.as_tensor(\n",
    "            np.array([[CHAR_TO_INDEX[char]]]),\n",
    "            dtype=torch.long) for char in sent]\n",
    "        state = None\n",
    "        for char in sent_as_tensor:\n",
    "            out, state = model.forward(char, train=False, state=state)\n",
    "        id = np.argmax(out.numpy().flatten())\n",
    "        if id == 27:\n",
    "            sent += ' '\n",
    "        else:\n",
    "            sent += ascii_lowercase[id - 1]\n",
    "    return sent\n",
    "\n",
    "# Обучение и генерация предложений на каждой эпохе\n",
    "vocab_size = 28\n",
    "embedding_dim = 28\n",
    "hidden_dim = 128\n",
    "\n",
    "model = TextGenerationNetwork(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "batch_size = 100\n",
    "n_epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "    start = time.time()\n",
    "    train_loss = 0.0\n",
    "    n_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "    for i in range(n_batches):\n",
    "        batch = X[i * batch_size: (i + 1) * batch_size]\n",
    "        X_batch = batch[:, :-1]\n",
    "        y_batch = batch[:, 1:].flatten()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred, _ = model.forward(X_batch)\n",
    "        loss = criterion(y_pred.view(-1, 28), y_batch)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= n_batches\n",
    "    train_losses.append(train_loss)\n",
    "    sec = time.time() - start\n",
    "    print(f'Epoch: {epoch}, time: {sec:.1f}s, train loss: {train_loss:.3f}')\n",
    "    print(generate_sentence(model) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e6240",
   "metadata": {},
   "source": [
    "В целом, код работает корректно и реализует задачу генерации текста на основе обученной RNN модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a4f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
